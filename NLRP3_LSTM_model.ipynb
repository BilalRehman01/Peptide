{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76da04a-de92-4897-91ff-5cf1a762992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming these are utility functions from LSTM_peptides.py\n",
    "from sample import _onehotencode, _sample_with_temp, _save_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c826fce-b0a7-44aa-8056-d6733137677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceHandler:\n",
    "    def __init__(self, window=0, step=1, refs=True):\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "        self.refs = refs\n",
    "        self.sequences = []\n",
    "        self.vocab = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.generated = None\n",
    "\n",
    "    def load_sequences(self, infile):\n",
    "        with open(infile, \"r\") as f:\n",
    "            self.sequences = [line.strip() for line in f.readlines()]\n",
    "        self.vocab = sorted(set(\"\".join(self.sequences)))\n",
    "\n",
    "    def analyze_training(self):\n",
    "        print(\"Number of sequences loaded: %d\" % len(self.sequences))\n",
    "        print(\"Vocabulary: \", self.vocab)\n",
    "\n",
    "    def pad_sequences(self, padlen=0):\n",
    "        maxlen = max([len(seq) for seq in self.sequences])\n",
    "        padlen = padlen if padlen > 0 else maxlen\n",
    "        self.sequences = [seq.ljust(padlen) for seq in self.sequences]\n",
    "\n",
    "    def one_hot_encode(self, target='all'):\n",
    "        self.X = np.array([_onehotencode(seq)[0] for seq in self.sequences])\n",
    "        self.y = np.array([_onehotencode(seq)[1] for seq in self.sequences])\n",
    "\n",
    "    def analyze_generated(self, num, fname, plot=True):\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(\"\\n\".join(self.generated))\n",
    "\n",
    "    def save_generated(self, logdir, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"\\n\".join(self.generated))\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n_vocab, outshape, session_name, n_units=64, batch=128, layers=2, cell=\"LSTM\",\n",
    "                 loss='categorical_crossentropy', lr=0.01, dropoutfract=0.1, l2_reg=None, ask=False, seed=42):\n",
    "        self.n_vocab = n_vocab\n",
    "        self.outshape = outshape\n",
    "        self.session_name = session_name\n",
    "        self.n_units = n_units\n",
    "        self.batch = batch\n",
    "        self.layers = layers\n",
    "        self.cell = cell\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.dropoutfract = dropoutfract\n",
    "        self.l2_reg = l2_reg\n",
    "        self.ask = ask\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.logdir = \"./\" + self.session_name\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        if self.cell == \"LSTM\":\n",
    "            cell_type = LSTM\n",
    "        else:\n",
    "            cell_type = GRU\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(cell_type(self.n_units, input_shape=(None, self.n_vocab), return_sequences=(self.layers > 1)))\n",
    "\n",
    "        for _ in range(1, self.layers):\n",
    "            model.add(cell_type(self.n_units, return_sequences=(_ < self.layers - 1)))\n",
    "\n",
    "        model.add(Dense(self.outshape, activation=\"softmax\"))\n",
    "\n",
    "        optimizer = Adam(learning_rate=self.lr)\n",
    "        model.compile(optimizer=optimizer, loss=self.loss)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, X, y, epochs=100, valsplit=0.2, sample=0):\n",
    "        checkpoint = ModelCheckpoint(filepath=self.logdir + '/checkpoint/model_epoch_{epoch:02d}.hdf5', save_best_only=True)\n",
    "        self.model.fit(X, y, batch_size=self.batch, epochs=epochs, validation_split=valsplit, callbacks=[checkpoint])\n",
    "\n",
    "    def plot_losses(self):\n",
    "        # Assuming you want to plot the training loss\n",
    "        plt.plot(self.model.history.history['loss'])\n",
    "        plt.plot(self.model.history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def sample(self, num=100, minlen=7, maxlen=50, start=None, temp=2.5, show=False):\n",
    "        sampled = []\n",
    "        lcntr = 0\n",
    "        for rs in tqdm(range(num)):\n",
    "            random.seed(rs)\n",
    "            if not maxlen:\n",
    "                longest = np.random.randint(7, 50)\n",
    "            else:\n",
    "                longest = maxlen\n",
    "\n",
    "            if start:\n",
    "                start_aa = start\n",
    "            else:\n",
    "                start_aa = 'B'\n",
    "            sequence = start_aa\n",
    "\n",
    "            while sequence[-1] != ' ' and len(sequence) <= longest:\n",
    "                x, _, _ = _onehotencode(sequence)\n",
    "                preds = self.model.predict(x)[0][-1]\n",
    "                next_aa = _sample_with_temp(preds, temp=temp)\n",
    "                sequence += self.vocab[next_aa]\n",
    "\n",
    "            if start_aa == 'B':\n",
    "                sequence = sequence[1:].rstrip()\n",
    "            else:\n",
    "                sequence = sequence.rstrip()\n",
    "\n",
    "            if len(sequence) < minlen:\n",
    "                lcntr += 1\n",
    "                continue\n",
    "\n",
    "            sampled.append(sequence)\n",
    "            if show:\n",
    "                print(sequence)\n",
    "\n",
    "        print(\"\\t%i sequences were shorter than %i\" % (lcntr, minlen))\n",
    "        return sampled\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        self.model.load_weights(filename)\n",
    "\n",
    "    def finetuneinit(self, new_session_name):\n",
    "        self.session_name = new_session_name\n",
    "        self.logdir = \"./\" + self.session_name\n",
    "        if not os.path.exists(self.logdir):\n",
    "            os.makedirs(self.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1f5e6-8d4c-4d77-9630-7e502a042ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for pretraining\n",
    "sessname = \"train100\"\n",
    "infile = \"new_sequences.csv\"\n",
    "neurons = 64 # 128, 512\n",
    "layers = 2   #3,\n",
    "epochs = 100 #150, 200, 300\n",
    "batchsize = 128 # 100, 512, 1000\n",
    "window = 0\n",
    "step = 1\n",
    "target = 'all'\n",
    "valsplit = 0.2\n",
    "dropout = 0.1\n",
    "learningrate = 0.01\n",
    "l2_rate = None\n",
    "\n",
    "# Loading sequence data, analyze, pad and encode it\n",
    "data = SequenceHandler(window=window, step=step)\n",
    "print(\"Loading sequences...\")\n",
    "data.load_sequences(infile)\n",
    "data.analyze_training()\n",
    "\n",
    "# Pad sequences\n",
    "print(\"\\nPadding sequences...\")\n",
    "data.pad_sequences(padlen=0)\n",
    "\n",
    "# One-hot encode padded sequences\n",
    "print(\"One-hot encoding sequences...\")\n",
    "data.one_hot_encode(target=target)\n",
    "\n",
    "# Building the LSTM model\n",
    "print(\"\\nBuilding model...\")\n",
    "model = Model(n_vocab=len(data.vocab), outshape=len(data.vocab), session_name=sessname, \n",
    "              n_units=neurons, batch=batchsize, layers=layers, cell=\"LSTM\", \n",
    "              loss='categorical_crossentropy', lr=learningrate, dropoutfract=dropout, \n",
    "              l2_reg=l2_rate, ask=True, seed=42)\n",
    "print(\"Model built!\")\n",
    "\n",
    "# Training model on data\n",
    "print(\"\\nTRAINING MODEL FOR {} EPOCHS...\\n\".format(epochs))\n",
    "model.train(data.X, data.y, epochs=epochs, valsplit=valsplit, sample=0)\n",
    "model.plot_losses()  # Plot loss\n",
    "\n",
    "# Save the model instance\n",
    "save_model_instance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4743a44-3a87-47c1-94e6-8c1aea590b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for sampling\n",
    "sample_name = \"testsample\"\n",
    "modfile = \"pretrained_model/checkpoint/model_epoch_67.hdf5\"\n",
    "sample_size = 100\n",
    "temperature = 2.5\n",
    "sample_length = 36\n",
    "\n",
    "print(\"\\nUSING PRETRAINED MODEL... ({})\\n\".format(modfile))\n",
    "model = load_model_instance(modfile)\n",
    "model.load_model(modfile)\n",
    "\n",
    "# Generating new data through sampling\n",
    "print(\"\\nSAMPLING {} SEQUENCES...\\n\".format(sample_size))\n",
    "data.generated = model.sample(sample_size, start='B', maxlen=sample_length, show=False, temp=temperature)\n",
    "data.analyze_generated(sample_size, fname=model.logdir + '/analysis_temp' + str(temperature) + '.txt', plot=True)\n",
    "data.save_generated(model.logdir, model.logdir + '/sampled_sequences_temp' + str(temperature) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e2615-c228-445b-a4c9-ce5fbef6314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for finetuning\n",
    "finetune_name = \"finetune10\"\n",
    "finetune_file = \"finetune_set.csv\"\n",
    "finetune_epochs = 10\n",
    "\n",
    "print(\"\\nUSING PRETRAINED MODEL FOR FINETUNING... ({})\\n\".format(modfile))\n",
    "print(\"Loading model...\")\n",
    "model = load_model_instance(modfile)\n",
    "model.load_model(modfile)\n",
    "model.finetuneinit(finetune_name)  # Generate new session folders for finetuning run\n",
    "\n",
    "print(\"Finetuning model...\")\n",
    "model.train(data.X, data.y, epochs=finetune_epochs, valsplit=valsplit, sample=0)\n",
    "model.plot_losses()  # Plot loss\n",
    "\n",
    "# Save the finetuned model instance\n",
    "save_model_instance(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
